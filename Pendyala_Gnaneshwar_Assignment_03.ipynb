{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gnani0825/Gnaneshwar_INFO5731_Fall2024/blob/main/Pendyala_Gnaneshwar_Assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from scratch instead of using any pre-existing libraries to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "rbw5sFirBulL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'gnaneshwar.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "documents = df['review'].dropna().tolist()\n",
        "def get_ngrams(text, n=3):\n",
        "    words = text.lower().split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "al_trigramms = []\n",
        "for doc in documents:\n",
        "    al_trigramms.extend(get_ngrams(doc))\n",
        "\n",
        "# Countingg  the frequency of all trigrams\n",
        "tr_counts = Counter(al_trigramms)\n",
        "print(tr_counts)\n",
        "\n",
        "\n",
        "def get_bigrams(text):\n",
        "    words = text.lower().split()\n",
        "    bigrams = zip(words, words[1:])\n",
        "    return [' '.join(bigram) for bigram in bigrams]\n",
        "all_bigrams = []\n",
        "for doc in documents:\n",
        "    all_bigrams.extend(get_bigrams(doc))\n",
        "def get_unigrams(text):\n",
        "    words = text.lower().split()\n",
        "    return words\n",
        "all_unigrams = []\n",
        "for doc in documents:\n",
        "    all_unigrams.extend(get_unigrams(doc))\n",
        "unigram_cou = Counter(all_unigrams)\n",
        "bigram_cou= Counter(all_bigrams)\n",
        "bigram_probabilities = {}\n",
        "for bigram, count in bigram_cou.items():\n",
        "    word1 = bigram.split()[0]\n",
        "    word2 = bigram.split()[1]\n",
        "    word1_count = unigram_cou[word1]\n",
        "    bigram_probabilities[bigram] = count / word1_count if word1_count > 0 else 0\n",
        "from itertools import islice\n",
        "print(\"\\nBigram probabil:\")\n",
        "for bigram, probability in islice(bigram_probabilities.items(), 100):\n",
        "    print(f\"{bigram}: {probability:.4f}\")\n"
      ],
      "metadata": {
        "id": "q0XjubmYB3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_noun_phrases(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged = pos_tag(words)\n",
        "    chunked = ne_chunk(tagged)\n",
        "\n",
        "    noun_phrz = []\n",
        "    for subtree in chunked:\n",
        "        if isinstance(subtree, nltk.Tree):  # If it is a noun phrase\n",
        "            noun_phrz.append(' '.join(word for word, tag in subtree))\n",
        "    return noun_phrz\n",
        "# Extracting noun phrases\n",
        "al_non_phrases = []\n",
        "for doc in documents:\n",
        "    al_non_phrases.extend(extract_noun_phrases(doc))\n",
        "noun_phrz_cou = Counter(al_non_phrases)\n",
        "max_noun_phrase_count = max(noun_phrz_cou.values())\n",
        "rel_freq = defaultdict(list)\n",
        "for doc in documents:\n",
        "    doc_noun_phrases = extract_noun_phrases(doc)\n",
        "    doc_noun_phrase_count = Counter(doc_noun_phrases)\n",
        "    for noun_phrase, count in noun_phrz_cou.items():\n",
        "        relative_freq = doc_noun_phrase_count[noun_phrase] / max_noun_phrase_count if max_noun_phrase_count > 0 else 0\n",
        "        rel_freq[noun_phrase].append(relative_freq)\n",
        "\n",
        "# Converting the df to tableform\n",
        "rel_freq_df = pd.DataFrame(rel_freq)\n",
        "print(\"\\nRelative Frequency of Noun Phrases for each Review:\")\n",
        "print(rel_freq_df.head(3))"
      ],
      "metadata": {
        "id": "e7ShfjACCSsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any pre-existing libraries to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjN0iysvo9-n"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "import math\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "file_path = 'gnaneshwar.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "documents = df['review'].dropna().tolist()\n",
        "tf = []\n",
        "for doc in documents:\n",
        "    words = doc.lower().split()\n",
        "    doc_tf = defaultdict(int)\n",
        "    for word in words:\n",
        "        doc_tf[word] += 1\n",
        "    tf.append({word: count / len(words) for word, count in doc_tf.items()})\n",
        "df = defaultdict(int)\n",
        "for doc_tf in tf:\n",
        "    for word in doc_tf:\n",
        "        df[word] += 1\n",
        "idf = {word: math.log(len(documents) / df[word]) for word in df}\n",
        "# Calculating  TF-IDF\n",
        "tf_idf_matrix = []\n",
        "for doc_tf in tf:\n",
        "    doc_tf_idf = {word: doc_tf[word] * idf[word] for word in doc_tf}\n",
        "    tf_idf_matrix.append(doc_tf_idf)\n",
        "for i, tf_idf in enumerate(tf_idf_matrix[:100]):\n",
        "    print(f\"Document {i+1}: {tf_idf}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"An outstanding movie with a haunting performance and best character development\"\n",
        "query_wor = query.lower().split()\n",
        "query_tf = defaultdict(int)\n",
        "for word in query_wor:\n",
        "    query_tf[word] += 1\n",
        "query_tf = {word: count / len(query_wor) for word, count in query_tf.items()}\n",
        "query_tf_idf = {word: query_tf[word] * idf.get(word, 0) for word in query_tf}\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    intersek = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum(vec1[word] * vec2[word] for word in intersek)\n",
        "    sum1 = sum(val**2 for val in vec1.values())\n",
        "    sum2 = sum(val**2 for val in vec2.values())\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "    return numerator / denominator if denominator != 0 else 0\n",
        "#Ranking documents by cosine similarity\n",
        "simmlar = []\n",
        "for i, doc_tf_idf in enumerate(tf_idf_matrix):\n",
        "    sim = cosine_similarity(query_tf_idf, doc_tf_idf)\n",
        "    simmlar.append((i, sim))\n",
        "ranked_docs = sorted(simmlar, key=lambda x: x[1], reverse=True)[:100]\n",
        "print(\"Document rankings based on query similarity:\")\n",
        "for doc_index, sim in ranked_docs:\n",
        "    print(f\"Document {doc_index + 1}: Similarity = {sim:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KRFdxDqC9W4",
        "outputId": "7e7ef37c-fed9-46d4-c0a8-7165ef535c59"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document rankings based on query similarity:\n",
            "Document 1081: Similarity = 0.1891\n",
            "Document 786: Similarity = 0.1614\n",
            "Document 435: Similarity = 0.1612\n",
            "Document 1061: Similarity = 0.1477\n",
            "Document 845: Similarity = 0.1357\n",
            "Document 1178: Similarity = 0.1350\n",
            "Document 1015: Similarity = 0.1322\n",
            "Document 1596: Similarity = 0.1218\n",
            "Document 709: Similarity = 0.1218\n",
            "Document 986: Similarity = 0.1181\n",
            "Document 674: Similarity = 0.1161\n",
            "Document 228: Similarity = 0.1146\n",
            "Document 946: Similarity = 0.1144\n",
            "Document 1242: Similarity = 0.1134\n",
            "Document 876: Similarity = 0.1133\n",
            "Document 400: Similarity = 0.1075\n",
            "Document 395: Similarity = 0.1059\n",
            "Document 1362: Similarity = 0.1022\n",
            "Document 1456: Similarity = 0.1018\n",
            "Document 470: Similarity = 0.0992\n",
            "Document 1108: Similarity = 0.0989\n",
            "Document 1475: Similarity = 0.0959\n",
            "Document 1408: Similarity = 0.0955\n",
            "Document 1588: Similarity = 0.0954\n",
            "Document 1355: Similarity = 0.0945\n",
            "Document 1244: Similarity = 0.0923\n",
            "Document 867: Similarity = 0.0919\n",
            "Document 1445: Similarity = 0.0919\n",
            "Document 461: Similarity = 0.0892\n",
            "Document 1236: Similarity = 0.0881\n",
            "Document 1347: Similarity = 0.0874\n",
            "Document 1209: Similarity = 0.0853\n",
            "Document 1421: Similarity = 0.0838\n",
            "Document 1258: Similarity = 0.0834\n",
            "Document 1310: Similarity = 0.0824\n",
            "Document 1442: Similarity = 0.0822\n",
            "Document 779: Similarity = 0.0817\n",
            "Document 634: Similarity = 0.0810\n",
            "Document 979: Similarity = 0.0790\n",
            "Document 292: Similarity = 0.0788\n",
            "Document 334: Similarity = 0.0779\n",
            "Document 972: Similarity = 0.0775\n",
            "Document 1560: Similarity = 0.0774\n",
            "Document 1105: Similarity = 0.0771\n",
            "Document 156: Similarity = 0.0767\n",
            "Document 1113: Similarity = 0.0761\n",
            "Document 473: Similarity = 0.0756\n",
            "Document 810: Similarity = 0.0756\n",
            "Document 1017: Similarity = 0.0743\n",
            "Document 1539: Similarity = 0.0736\n",
            "Document 975: Similarity = 0.0729\n",
            "Document 1290: Similarity = 0.0721\n",
            "Document 1338: Similarity = 0.0706\n",
            "Document 468: Similarity = 0.0705\n",
            "Document 439: Similarity = 0.0700\n",
            "Document 699: Similarity = 0.0686\n",
            "Document 385: Similarity = 0.0682\n",
            "Document 469: Similarity = 0.0682\n",
            "Document 888: Similarity = 0.0679\n",
            "Document 510: Similarity = 0.0678\n",
            "Document 592: Similarity = 0.0676\n",
            "Document 1187: Similarity = 0.0676\n",
            "Document 1435: Similarity = 0.0669\n",
            "Document 465: Similarity = 0.0666\n",
            "Document 662: Similarity = 0.0658\n",
            "Document 929: Similarity = 0.0646\n",
            "Document 482: Similarity = 0.0639\n",
            "Document 702: Similarity = 0.0630\n",
            "Document 736: Similarity = 0.0624\n",
            "Document 545: Similarity = 0.0621\n",
            "Document 1032: Similarity = 0.0617\n",
            "Document 591: Similarity = 0.0612\n",
            "Document 959: Similarity = 0.0609\n",
            "Document 1036: Similarity = 0.0608\n",
            "Document 166: Similarity = 0.0608\n",
            "Document 376: Similarity = 0.0603\n",
            "Document 582: Similarity = 0.0599\n",
            "Document 1132: Similarity = 0.0594\n",
            "Document 151: Similarity = 0.0592\n",
            "Document 1043: Similarity = 0.0590\n",
            "Document 1650: Similarity = 0.0585\n",
            "Document 153: Similarity = 0.0584\n",
            "Document 1307: Similarity = 0.0579\n",
            "Document 1067: Similarity = 0.0577\n",
            "Document 1300: Similarity = 0.0576\n",
            "Document 1320: Similarity = 0.0576\n",
            "Document 1008: Similarity = 0.0575\n",
            "Document 1117: Similarity = 0.0574\n",
            "Document 193: Similarity = 0.0570\n",
            "Document 26: Similarity = 0.0570\n",
            "Document 1129: Similarity = 0.0570\n",
            "Document 938: Similarity = 0.0569\n",
            "Document 36: Similarity = 0.0569\n",
            "Document 478: Similarity = 0.0563\n",
            "Document 188: Similarity = 0.0561\n",
            "Document 771: Similarity = 0.0560\n",
            "Document 1215: Similarity = 0.0560\n",
            "Document 490: Similarity = 0.0560\n",
            "Document 928: Similarity = 0.0559\n",
            "Document 257: Similarity = 0.0558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eczZgyAoo05Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d294d733-2d95-4442-889a-0edf715389ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of tokenized and preprocessed data: [['bet', 'youd', 'never', 'think', 'mashup', 'heavyhanded', 'rambo', 'mystical', 'stunning', 'elegance', 'crouching', 'tiger', 'would', 'work', 'especially', 'mixing', 'several', 'musical', 'numbers', 'think', 'would', 'wrong', 'movie', 'top', 'every', 'way', 'pure', 'awesomeness', 'even', '3hour', 'runtime', 'engrossed', 'great', 'story', 'thorough', 'character', 'development', 'young', 'native', 'girl', 'effectively', 'purchased', 'mean', 'british', 'government', 'controlling', 'people', 'malli', 'presumably', 'india', 'small', 'tribe', 'belongs', 'talented', 'fighter', 'stop', 'absolutely', 'nothing', 'get', 'back', 'british', 'government', 'also', 'indian', 'soldier', 'equally', 'skilled', 'dwarf', 'soldiers', 'youll', 'see', 'spectacular', 'opening', 'scene', 'two', 'skilled', 'fighters', 'clash', 'come', 'together', 'heritage', 'roots', 'duty', 'one', 'hold', 'true', 'army', 'works', 'movie', 'blew', 'away', 'ive', 'never', 'seen', 'special', 'effects', 'quite', 'like', 'plethora', 'frames', 'could', 'used', 'sweet', 'posters', 'movie', 'produced', 'smart', 'editing', 'cinematography', 'choreography', 'modern', 'must', 'see'], ['pushed', 'play', 'really', 'believe', 'would', 'ever', 'watch', 'whole', 'movie', 'wanted', 'take', 'look', 'bail', 'itwell', 'wrong', 'movie', 'freaken', 'masterpiece', 'combines', 'extraordinary', 'drama', 'stunning', 'stunts', 'action', 'great', 'photography', 'absolutely', 'beautiful', 'music', 'given', 'bollywood', 'movie', 'know', 'going', 'watch', 'pretty', 'ridiculous', 'action', 'scenes', 'overall', 'cheesy', 'direction', 'dialogues', 'acting', 'wont', 'exactly', 'oscar', 'worthy', 'none', 'made', 'enjoy', 'movie', 'time', 'passed', 'easily', 'ended', 'really', 'great', 'timethe', 'eye', 'candy', 'ram', 'charan', 'made', 'movie', 'even', 'watch', 'worthy', 'particular', 'one', 'ridiculously', 'handsome', 'man', 'say', 'p'], ['try', 'review', 'without', 'comparing', 'anything', 'directly', 'really', 'comparable', 'action', 'blockbusters', 'disappointed', 'recently', 'say', 'largescale', 'bigbudget', 'action', 'movies', 'right', 'hollywood', 'isnt', 'incapable', 'making', 'movies', 'deliver', 'excitement', 'emotion', 'many', 'pale', 'comparison', 'rrr', 'without', 'pointing', 'movies', 'particular', 'dont', 'know', 'whats', 'directly', 'comparable', 'hollywood', 'take', 'notesrrr', 'many', 'familiar', 'tropes', 'beats', 'get', 'historical', 'epicsaction', 'movies', 'uses', 'tropes', 'well', 'things', 'weve', 'seen', 'onscreen', 'dozens', 'times', 'still', 'exciting', 'entertaining', 'theyre', 'used', 'properly', 'rrr', 'testament', 'thatthe', 'amazing', 'action', 'probably', 'stands', 'core', 'film', 'also', 'really', 'good', 'story', 'heroes', 'want', 'see', 'win', 'villains', 'want', 'see', 'defeated', 'theres', 'extra', 'conflict', 'two', 'main', 'heroes', 'much', 'movie', 'ultimately', 'good', 'vs', 'evil', 'story', 'thats', 'pretty', 'straightforward', 'honest', 'thanks', 'great', 'characters', 'strong', 'performances', 'ends', 'enoughtheres', 'little', 'way', 'slow', 'scenes', 'dead', 'air', 'another', 'reason', 'three', 'hour', 'runtime', 'flies', 'action', 'good', 'complain', 'lacklustre', 'action', 'modern', 'action', 'movies', 'lot', 'really', 'happy', 'find', 'rrr', 'action', 'well', 'amazing', 'stunts', 'great', 'setups', 'big', 'set', 'piece', 'scenes', 'level', 'brutality', 'makes', 'feel', 'impact', 'combat', 'much', 'feels', 'gratuitous', 'way', 'making', 'things', 'overthetop', 'best', 'way', 'possible', 'point', 'feels', 'like', 'rules', 'consequences', 'good', 'guys', 'two', 'main', 'heroes', 'almost', 'superheroes', 'arguably', 'makes', 'rrr', 'best', 'superhero', 'film', 'yearsexcellent', 'stuff', 'couple', 'lesser', 'performances', 'minor', 'characters', 'occasionally', 'clunky', 'english', 'dialogue', 'british', 'characters', 'could', 'criticise', 'theyre', 'nitpicks', 'great', 'action', 'movie', 'epic', 'warrants', 'three', 'hour', 'runtime'], ['seen', 'lot', 'movies', 'time', 'made', 'lot', 'different', 'styles', 'different', 'genres', 'around', 'world', 'ive', 'seen', 'everything', 'mainstream', 'movie', 'imaginable', 'experimental', 'cant', 'even', 'remember', 'last', 'time', 'came', 'away', 'movie', 'thinking', 'id', 'never', 'seen', 'anything', 'like', 'thats', 'felt', 'rrrthis', 'movie', 'much', 'muchness', 'may', 'turn', 'almost', 'wife', 'nearly', 'bailed', '20', 'minute', 'mark', 'film', 'top', 'ridiculous', 'got', 'hooked', 'totally', 'ride', 'point', 'disappointed', '3hour', 'behemoth', 'endingdo', 'like', 'see', 'musclebound', 'slickedup', 'men', 'fighting', 'tigers', 'check', 'public', 'floggings', 'turn', 'musical', 'numbers', 'got', 'evil', 'british', 'people', 'extremely', 'evil', 'england', 'sue', 'filmmakers', 'defamation', 'sure', 'evil', 'british', 'people', 'mauled', 'rampaging', 'jungle', 'animals', 'betcha', 'beheadings', 'yep', 'romance', 'course', 'homoeroticism', 'intense', 'watching', 'movie', 'may', 'turn', 'gay', 'hooboy', 'lets', 'say', 'anything', 'already', 'movie', 'isnt', 'worth', 'anywaywatch', 'rrr', 'make', 'sure', 'whatever', 'first', 'movie', 'watch', 'one', 'dont', 'care', 'much', 'certainly', 'feel', 'like', 'palest', 'imitation', 'movie', 'ever', 'seen', 'seriously', 'movie', 'deliriously', 'bonkers', 'unafraid', 'absolutely', 'absurd', 'almost', 'impossible', 'watch', 'pretty', 'much', 'movie', 'disappointed', 'one', 'thanks', 'lot', 'rrrgrade'], ['incredible', 'film', 'never', 'heard', 'film', 'netflix', 'brought', 'saw', 'clips', 'movie', 'decided', 'looked', 'pretty', 'good', 'watched', 'glad', 'dancing', 'scene', 'party', 'incredible', 'arguably', 'scene', 'best', 'scene', 'film', 'also', 'first', 'scene', 'ram', 'charan', 'takes', 'mob', 'people', 'pretty', 'awesome', 'watch', 'well', 'overall', 'definitely', 'one', 'best', 'films', 'year', 'combining', 'action', 'comedy', 'romance', 'dancing', 'great', 'storytelling']]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "file_path = 'gnaneshwar.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "data = df['review'].dropna().tolist()\n",
        "def preprocess_text(data):\n",
        "    stop_wor = set(stopwords.words('english'))\n",
        "    sent = []\n",
        "    for text in data:\n",
        "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "        words = [word for word in word_tokenize(text) if word not in stop_wor]\n",
        "        sent.append(words)\n",
        "    return sent\n",
        "processed_data = preprocess_text(data)\n",
        "sam_data = processed_data[:5]\n",
        "print(\"Sample of tokenized and preprocessed data:\", sam_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=processed_data, vector_size=300, window=5, min_count=1, sg=1, epochs=100)\n",
        "model.save(\"word_embedding_model.model\")\n",
        "print(\"Model trained and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4bmUZy1Dnve",
        "outputId": "d6b39b45-52ae-4e51-a8fa-1aa370d5f0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import Word2Vec\n",
        "wor = list(model.wv.index_to_key)\n",
        "word_vec = np.array([model.wv[word] for word in wor])\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "word_vec_2d = tsne.fit_transform(word_vec)\n",
        "#Plotting the word embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(word_vec_2d[:, 0], word_vec_2d[:, 1], edgecolors='k', c='r')\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (word_vec_2d[i, 0], word_vec_2d[i, 1]), fontsize=9)\n",
        "\n",
        "plt.title('Word2Vec - 2D visualization of Word Embeddings', fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cgVuZQPiEFNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation data for sentiment analysis.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyK54UY6ompS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "# Link:https://github.com/Gnani0825/Pendyala_Gnaneshwar_ASSIGN_04_CSV/blob/main/anil_reviews.csv\n",
        "#\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXlsbrirHRo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# it is very challenging one though i had the little idea about the concepts it made me to do a lot of reasearch about the topics and i was new to this machine learning concepts so it was very interesting to know new things and i am more exited to look in depth concepts and excel in the subject future\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}