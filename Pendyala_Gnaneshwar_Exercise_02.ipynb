{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoMtmaPmlXQpIuUobdIRlu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gnani0825/Gnaneshwar_INFO5731_Fall2024/blob/main/Pendyala_Gnaneshwar_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "o-SM6OsYynPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "    \"Accept\": \"text/html\",\n",
        "    \"Connection\": \"keep-alive\"\n",
        "}\n",
        "\n",
        "all_data=[]# Initialize a list to store all rows of data\n",
        "\n",
        "for i in range(1, 180):\n",
        "    url = f'https://www.census2011.co.in/district.php?page={i}'\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response_result = BeautifulSoup(response.content, 'html.parser')\n",
        "    tables = response_result.find_all('table', class_='filter table table-striped table-hover')\n",
        "\n",
        "    for table in tables:\n",
        "        # Extract headers\n",
        "        country_titles = table.find_all('th')\n",
        "        country_table_titles = [title.text.strip() for title in country_titles]\n",
        "\n",
        "        # Extract rows\n",
        "        rows = table.find_all('tr')\n",
        "        for row in rows[1:]:  # Skip header row\n",
        "            row_data = row.find_all('td')\n",
        "            individual_rowdata = [data.text.strip() for data in row_data]\n",
        "            all_data.append(individual_rowdata)\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df = pd.DataFrame(all_data, columns=country_table_titles)\n",
        "df.to_csv('gnaneshwarpendyala.csv')\n",
        "# Print or save the DataFrame\n",
        "print(df)\n",
        "# Optionally save to a CSV file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvlkDiUJIQZB",
        "outputId": "323b62ff-c0b5-4346-f14e-72c2f0e6de13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       #                    District                        State  Population  \\\n",
            "0      1                       Thane                  Maharashtra  11,060,148   \n",
            "1      2  North Twenty Four Parganas                  West Bengal  10,009,781   \n",
            "2      3                   Bangalore                    Karnataka   9,621,551   \n",
            "3      4                        Pune                  Maharashtra   9,429,408   \n",
            "4      5             Mumbai Suburban                  Maharashtra   9,356,962   \n",
            "..   ...                         ...                          ...         ...   \n",
            "635  636                    Nicobars  Andaman and Nicobar Islands      36,842   \n",
            "636  637                 Upper Siang            Arunachal Pradesh      35,320   \n",
            "637  638             Lahul and Spiti             Himachal Pradesh      31,564   \n",
            "638  639                       Anjaw            Arunachal Pradesh      21,167   \n",
            "639  640               Dibang Valley            Arunachal Pradesh       8,004   \n",
            "\n",
            "       Growth Sex-Ratio Literacy  \n",
            "0     36.01 %       886    84.53  \n",
            "1     12.04 %       955    84.06  \n",
            "2     47.18 %       916    87.67  \n",
            "3     30.37 %       915    86.15  \n",
            "4      8.29 %       860    89.91  \n",
            "..        ...       ...      ...  \n",
            "635  -12.42 %       777    78.06  \n",
            "636    5.87 %       889    59.99  \n",
            "637   -5.00 %       903    76.81  \n",
            "638   14.19 %       839    56.46  \n",
            "639   10.07 %       813    64.10  \n",
            "\n",
            "[640 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "ULwG-yq3ywjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "sample_data = df.sample(n=600)\n",
        "print(sample_data)\n",
        "print(sample_data.head())"
      ],
      "metadata": {
        "id": "QGmA2zqpyw5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f2eacc-a9cf-4202-e610-127cf720c076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       #      District          State Population   Growth Sex-Ratio Literacy\n",
            "112  113        Deoria  Uttar Pradesh  3,100,946  14.31 %      1017    71.13\n",
            "298  299       Kannauj  Uttar Pradesh  1,656,616  19.27 %       879    72.70\n",
            "296  297   Chitradurga      Karnataka  1,659,456   9.33 %       974    73.71\n",
            "117  118       Udaipur      Rajasthan  3,068,420  23.69 %       958    61.82\n",
            "430  431    Chandigarh     Chandigarh  1,055,450  17.19 %       818    86.05\n",
            "..   ...           ...            ...        ...      ...       ...      ...\n",
            "480  481       Wayanad         Kerala    817,420   4.71 %      1035    89.03\n",
            "41    42          Gaya          Bihar  4,391,418  26.43 %       937    63.67\n",
            "507  508     Jaisalmer      Rajasthan    669,919  31.81 %       852    57.22\n",
            "495  496  The Nilgiris     Tamil Nadu    735,394  -3.51 %      1042    85.20\n",
            "28    29       Chennai     Tamil Nadu  4,646,732   6.98 %       989    90.18\n",
            "\n",
            "[600 rows x 7 columns]\n",
            "       #     District          State Population   Growth Sex-Ratio Literacy\n",
            "112  113       Deoria  Uttar Pradesh  3,100,946  14.31 %      1017    71.13\n",
            "298  299      Kannauj  Uttar Pradesh  1,656,616  19.27 %       879    72.70\n",
            "296  297  Chitradurga      Karnataka  1,659,456   9.33 %       974    73.71\n",
            "117  118      Udaipur      Rajasthan  3,068,420  23.69 %       958    61.82\n",
            "430  431   Chandigarh     Chandigarh  1,055,450  17.19 %       818    86.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ],
      "metadata": {
        "id": "JDFTCB-AzAZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url=\"https://dl.acm.org/action/doSearch?AllField=articles\"\n",
        "response=requests.get(url)\n",
        "response=response.content\n",
        "soup=BeautifulSoup(response,'html.parser')\n",
        "context=soup.find_all('div',class_='issue-item__content-right' )\n",
        "for contexts in context:\n",
        "  articlename=contexts.find('h5',class_='issue-item__title').text\n",
        "  print(articlename)\n",
        "#authorname=contexts.find('ul',class_='rlist--inline loa truncate-list trunc-done')\n",
        "  conference=contexts.find('div',class_='issue-item__detail')\n",
        "  venues=conference.find('span',class_='epub-section__title').text\n",
        "  print(venues)\n",
        "  abstract=contexts.find('div',class_='issue-item__abstract truncate-text trunc-done')\n",
        "  print(abstract)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNIRymSt-l8u",
        "outputId": "253e1a6c-e45e-459f-d11d-ff2818002bda"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Design Patterns for Data-Driven News Articles\n",
            "CHI '24: Proceedings of the CHI Conference on Human Factors in Computing Systems\n",
            "None\n",
            "Mining the History Sections of Wikipedia Articles on Science and Technology\n",
            "JCDL '23: Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries\n",
            "None\n",
            "Drug Target Extraction from Biomedical Articles Based on a Two-Stage Cascading Framework\n",
            "JCDL '23: Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries\n",
            "None\n",
            "AI-Generated News Articles Based on Large Language Models\n",
            "AISNS '23: Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security\n",
            "None\n",
            "Co-Creating Question-and-Answer Style Articles with Large Language Models for Research Promotion\n",
            "DIS '24: Proceedings of the 2024 ACM Designing Interactive Systems Conference\n",
            "None\n",
            "Geographic Information Retrieval Using Wikipedia Articles\n",
            "WWW '23: Proceedings of the ACM Web Conference 2023\n",
            "None\n",
            "Automatic Quality Assessment of Wikipedia Articles—A Systematic Literature Review\n",
            "ACM Computing Surveys (CSUR), Volume 56, Issue 4\n",
            "None\n",
            "Implementation of Domain Adaptation for Keyword Determination of Scientific Articles Based on Multilabel BERT\n",
            "SIET '23: Proceedings of the 8th International Conference on Sustainable Information Engineering and Technology\n",
            "None\n",
            "Descartes: Generating Short Descriptions of Wikipedia Articles\n",
            "WWW '23: Proceedings of the ACM Web Conference 2023\n",
            "None\n",
            "Analysis of The Irish Times Newspaper Articles\n",
            "ICBDE '22: Proceedings of the 5th International Conference on Big Data and Education\n",
            "None\n",
            "Future Timelines: Extraction and Visualization of Future-related Content From News Articles\n",
            "WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining\n",
            "None\n",
            "Automatically Temporal Labeled Data Generation Using Positional Lexicon Expansion for Focus Time Estimation of News Articles\n",
            "ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), Volume 23, Issue 5\n",
            "None\n",
            "DynamicESG: A Dataset for Dynamically Unearthing ESG Ratings from News Articles\n",
            "CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management\n",
            "None\n",
            "Put Your Voice on Stage: Personalized Headline Generation for News Articles\n",
            "ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 18, Issue 3\n",
            "None\n",
            "A Category-agnostic Graph Attention-based Approach for Determining Notability of Articles for Wikipedia\n",
            "WWW '24: Companion Proceedings of the ACM Web Conference 2024\n",
            "None\n",
            "Classification of advertisement articles using sentiment analysis: (Research-based on Korean natural language processing and deep learning technology)\n",
            "NLPIR '22: Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval\n",
            "None\n",
            "Making Data-Driven Articles more Accessible: An Active Preference Learning Approach to Data Fact Personalization\n",
            "DIS '23: Proceedings of the 2023 ACM Designing Interactive Systems Conference\n",
            "None\n",
            "Cryptocurrency Price Prediction Using Twitter and News Articles Analysis\n",
            "IC3-2022: Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing\n",
            "None\n",
            "Automatic Extraction of Patterns in Digital News Articles of Femicides occurred in Mexico by Text Mining Techniques\n",
            "WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining\n",
            "None\n",
            "Domain Adaptation in Nested Named Entity Recognition From Scientific Articles in Agriculture\n",
            "SOICT '23: Proceedings of the 12th International Symposium on Information and Communication Technology\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ],
      "metadata": {
        "id": "iTRo6MFsyyTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import statvfs\n",
        "!pip install ntscraper\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from ntscraper import Nitter\n",
        "Headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "    \"Accept\": \"text/html\",\n",
        "    \"Connection\": \"keep-alive\"\n",
        "}\n",
        "\n",
        "ganesh=Nitter()\n",
        "tweets=ganesh.get_tweets('KTR',mode='hashtag',number=33)\n",
        "data=[]\n",
        "for i in tweets['tweets']:\n",
        "  tweetdata={\n",
        "      'Text':i['text'],\n",
        "      'Date':i['date'],\n",
        "      'Likes':i['stats']['likes'],\n",
        "      'Retweets':i['stats']['retweets']\n",
        "  }\n",
        "  data.append(tweetdata)\n",
        "  df=pd.DataFrame(data)\n",
        "print(df)\n",
        "#we can save the data formated as csv\n",
        "#df.to_csv('pendyala')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6VF7tgOryyon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01f6a6d-762b-4195-b806-bdc91e69726c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ntscraper\n",
            "  Downloading ntscraper-0.3.17-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.12.3)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.9.4)\n",
            "Requirement already satisfied: tqdm>=4.66 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->ntscraper) (2024.8.30)\n",
            "Downloading ntscraper-0.3.17-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: ntscraper\n",
            "Successfully installed ntscraper-0.3.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing instances: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
            "INFO:root:No instance specified, using random instance https://nitter.lucabased.xyz\n",
            "INFO:root:Current stats for KTR: 8 tweets, 0 threads...\n",
            "INFO:root:Current stats for KTR: 13 tweets, 0 threads...\n",
            "INFO:root:Current stats for KTR: 16 tweets, 0 threads...\n",
            "INFO:root:Current stats for KTR: 21 tweets, 0 threads...\n",
            "INFO:root:Current stats for KTR: 23 tweets, 0 threads...\n",
            "INFO:root:Current stats for KTR: 29 tweets, 0 threads...\n",
            "INFO:root:Current stats for KTR: 33 tweets, 0 threads...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Text  \\\n",
            "0   నీకు ఎట్లాగూ ఏదీ తెల్వదు..  నీ దరిద్రం ఏందంటే ...   \n",
            "1   Hands down the best smile in Kpop. I don’t car...   \n",
            "2   మమ్మల్ని తిట్టడం కాదు.. ఇకనైనా పాలన పై ఫోకస్ ప...   \n",
            "3   Shorts: రేవంత్ రెడ్డి శపథం చేసి మరీ..!! KTR Vs...   \n",
            "4   తెలంగాణ సచివాలయం ఎదురుగా తెలంగాణ తల్లి విగ్రహం...   \n",
            "5   On the occasion of National Integration Day on...   \n",
            "6   Hey #KTR u people must be kept out of Bharat, ...   \n",
            "7   Correct ga cheppav bro, ee #KTR gadu black lo ...   \n",
            "8        Ah comparison enti bro #KTR #AllHailTheTiger   \n",
            "9                              #Ktr @BRSparty @KTRBRS   \n",
            "10                            Absolutely #KTR @KTRBRS   \n",
            "11             ఈరోజు కేటీఆర్ @KTRBRS గారితో🔥🔥..  #KTR   \n",
            "12  ఎవరు ఏమన్నారు? వారి వాదనలు ఏంటి? #RevanthReddy...   \n",
            "13                                  Absolutely 💯 #KTR   \n",
            "14  జాతీయ జెండాను ఎగురవేసిన కేటీఆర్ #Tnews #KTR #T...   \n",
            "15  నీకు తెల్వదాయె..ఎవరన్నా చెప్తే వినవాయె గిట్లైత...   \n",
            "16  తెలంగాణ ప్రజలందరికీ  \"#జాతీయ_సమైక్యత_దినోత్సవం...   \n",
            "17  కంప్యూటర్_ను పరిచయం చేసింది రాజీవ్ గాంధీ అన్న ...   \n",
            "18  రాజీవ్ గాంధీ వల్లే ఐటీ శాఖ వచ్చిందా..?  #brsvs...   \n",
            "19  ఉచ్ఛలో చేపలు పట్టే రకం వీళ్ళూ..  రైతు భరోసా మీ...   \n",
            "20  जैसे ही हमारी सरकार आएगी हम #राजीव गांधी की प्...   \n",
            "21  కేటీఆర్ ఒక రేంజ్ లో సవాల్ KTR Open challenge t...   \n",
            "22  రేవంత్ రెడ్డి  నోరు అదుపులో పెట్టుకో.. Ktr Agg...   \n",
            "23  Swetcha Daily Epaper:  ఇది ఘోర అవమానం రాజీవ్ గ...   \n",
            "24  Jai Telangana   #TelanganaCulture #Telangana #...   \n",
            "25  It's not that you talk because you have a mout...   \n",
            "26  కుక్క కు బొక్క దొరికినట్లు ఉన్నది....!! పది పద...   \n",
            "27  Okko maata   Okko Bullet dimpunnattu cheparu #...   \n",
            "28  https://bit.ly/2El2IN6 #車いす道中記 です。#京都府 も色々南北問わ...   \n",
            "29  CM Revanth Reddy Comments on KCR and KTR | Raj...   \n",
            "30  Someone loves chanting the name of #KTR sir (a...   \n",
            "31  తెలంగాణ బిడ్డను ప్రధానమంత్రి చేసిన సోనియా గాంధ...   \n",
            "32  మీరు మీ #బీఆర్ఎస్ పార్టీ చెయ్యలేని పనులు మా #క...   \n",
            "\n",
            "                           Date  Likes  Retweets  \n",
            "0    Sep 17, 2024 · 5:51 AM UTC    911       193  \n",
            "1    Sep 16, 2024 · 5:59 PM UTC    123        45  \n",
            "2    Sep 17, 2024 · 9:35 AM UTC     14         4  \n",
            "3    Sep 17, 2024 · 6:35 PM UTC      0         0  \n",
            "4    Sep 17, 2024 · 6:26 AM UTC    146        53  \n",
            "5    Sep 17, 2024 · 5:22 PM UTC      0         0  \n",
            "6    Sep 17, 2024 · 5:17 PM UTC      1         0  \n",
            "7    Sep 17, 2024 · 5:13 PM UTC      0         0  \n",
            "8    Sep 17, 2024 · 5:00 PM UTC      2         0  \n",
            "9    Sep 17, 2024 · 4:54 PM UTC      1         0  \n",
            "10   Sep 17, 2024 · 3:57 PM UTC      0         0  \n",
            "11   Sep 17, 2024 · 3:55 PM UTC      1         0  \n",
            "12   Sep 17, 2024 · 3:53 PM UTC      3         0  \n",
            "13   Sep 17, 2024 · 2:27 PM UTC     11         0  \n",
            "14   Sep 17, 2024 · 6:19 AM UTC     13         4  \n",
            "15   Sep 17, 2024 · 7:39 AM UTC     22         8  \n",
            "16   Sep 17, 2024 · 1:39 PM UTC      1         0  \n",
            "17   Sep 17, 2024 · 1:30 PM UTC      0         0  \n",
            "18   Sep 17, 2024 · 1:22 PM UTC      0         0  \n",
            "19   Sep 13, 2024 · 7:03 AM UTC     10         6  \n",
            "20   Sep 17, 2024 · 1:04 PM UTC      1         0  \n",
            "21  Sep 17, 2024 · 12:35 PM UTC      0         0  \n",
            "22  Sep 17, 2024 · 12:33 PM UTC      0         0  \n",
            "23  Sep 17, 2024 · 11:51 AM UTC      0         0  \n",
            "24  Sep 17, 2024 · 11:24 AM UTC      1         0  \n",
            "25  Sep 17, 2024 · 11:15 AM UTC      1         0  \n",
            "26  Sep 17, 2024 · 11:08 AM UTC      0         0  \n",
            "27  Sep 17, 2024 · 11:03 AM UTC      3         0  \n",
            "28  Sep 17, 2024 · 10:57 AM UTC      0         0  \n",
            "29  Sep 17, 2024 · 10:48 AM UTC      0         0  \n",
            "30   Sep 16, 2024 · 3:28 PM UTC    143        15  \n",
            "31  Sep 16, 2024 · 12:37 PM UTC     11         4  \n",
            "32   Sep 16, 2024 · 2:50 PM UTC      9         2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ],
      "metadata": {
        "id": "gMnlJDtcyzLx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LdSwEkI6ICZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "BncIrJ_FzTGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "it was very challenging to me to finish the task as web scraping was new to me and the little python i learned was from previous exercises and overall i got an idea what is web scraping and what are the uses,i did a lot of research about scrapping and did at my best and there is difficulty in doing as it was new so i will improve by learning the concepts in future.\n",
        "'''"
      ],
      "metadata": {
        "id": "VCh7iJldzUf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}