{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzKu6QlQ0pzMUwNNqCsFx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gnani0825/Gnaneshwar_INFO5731_Fall2024/blob/main/Pendyala_Gnaneshwar_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ],
      "metadata": {
        "id": "dZU01KWc-szv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "yes i took the text classification task as product reviews\n",
        "1.Bog-0f-words\n",
        "by this we extract all the occurences of a word of a sententence by using  countvectorizer.the min drawback of this is\n",
        "2. TF-IDF\n",
        "TF means Term Frequency\n",
        "TF = [number of times word appeared / total no of words in a document]\n",
        "Term Frequency values ranges between 0 and 1. If a word occurs more number of times, then it's value will be close to 1.\n",
        "IDF stands for Inverse Document Frequency\n",
        "IDF = [log(Total number of documents / number of documents that contains the word)]\n",
        "In IDF, if a word occured  more times then it's value will be less and ratio will be close to 0.\n",
        "so,\n",
        "   TF-IDF = TermFrequency(TF) * InverseDocumentFrequency(IDF)\n",
        "by we this feature we can supress the reapeating words and weightage will be less for that words.\n",
        "3.sentiment lexicon\n",
        "we extract the positive or negative review based on words present by using sentowordnet\n",
        "there is a polarity score with the help of this we confirm wheater it is positive or negative.\n",
        "4.N-grams\n",
        "bigrams are extracted from the nltk library\n",
        "for example:the movie is great\n",
        "for n=2 biogram ( the movie,movie is,is great.)\n",
        "for n=3 trigram  (the movie is,movie is great)\n",
        "5.parts-of-speech tagging:\n",
        "by this we can allote grammatical meaning to words which helps in semantic analysis bby identifying adjectivs and verbs.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "AYeuLwLP-wac",
        "outputId": "97079346-570f-4a6f-c9a2-d592ce4acd7a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nyes i took the text classification task as product reviews\\n1.Bog-0f-words\\nby this we extract all the occurences of a word of a sententence by using  countvectorizer.the min drawback of this is\\n2. TF-IDF\\nTF means Term Frequency\\nTF = [number of times word appeared / total no of words in a document]\\nTerm Frequency values ranges between 0 and 1. If a word occurs more number of times, then it's value will be close to 1.\\nIDF stands for Inverse Document Frequency\\nIDF = [log(Total number of documents / number of documents that contains the word)]\\nIn IDF, if a word occured  more times then it's value will be less and ratio will be close to 0.\\nso,\\n   TF-IDF = TermFrequency(TF) * InverseDocumentFrequency(IDF)\\nby we this feature we can supress the reapeating words and weightage will be less for that words.\\n3.sentiment lexicon\\nwe extract the positive or negative review based on words present by using sentowordnet\\nthere is a polarity score with the help of this we confirm wheater it is positive or negative.\\n4.N-grams\\nbigrams are extracted from the nltk library\\nfor example:the movie is great\\nfor n=2 biogram ( the movie,movie is,is great.)\\nfor n=3 trigram  (the movie is,movie is great)\\n5.parts-of-speech tagging:\\nby this we can allote grammatical meaning to words which helps in semantic analysis bby identifying adjectivs and verbs.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ],
      "metadata": {
        "id": "dPWoVb-i_FYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "#download the packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "reviews = {'text':[\n",
        "    \"I highly recommend the VANMASS phone holder, and once you try one you will NEVER switch.\",\n",
        "    \"Very disappointed, not what I expected at all.\",\n",
        "    \"Product feels very high quality. The suction to mount was very sticky and don't feel it will come undone. While driving a semi truck, it doesn't shake as much as other products. Very easy to use. Just have to get used to the release button on the back side. Good price for good product. Overall very happy with the product.\",\n",
        "     \"Waste of money, doesn't work as advertised.\",\n",
        "    \"Fantastic performance, I absolutely love it!\"\n",
        "],\n",
        "  'label': ['positive', 'negative', 'positive', 'negative', 'positive']}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk0AsQ3r_GYu",
        "outputId": "db854022-b090-4593-a26e-f5641e30c2dc"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(reviews)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "b_featur = count_vect.fit_transform(X_train)\n",
        "#bag of words\n",
        "print(\"Bag of Words\", b_featur.toarray())\n",
        "t_vector= TfidfVectorizer(stop_words='english')\n",
        "tf_feat = t_vector.fit_transform(X_train)\n",
        "#td-idf\n",
        "print(\"TF-IDF Feature Matrix:\\n\", tf_feat .toarray())\n",
        "gram_vector = CountVectorizer(ngram_range=(2, 3), stop_words='english')\n",
        "#n-grams\n",
        "ngram_feat = gram_vector.fit_transform(X_train)\n",
        "print(\"N-grams Feature Matrix:\\n\", ngram_feat.toarray())\n",
        "#pos tag\n",
        "def pos_feat(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "    pos_conts = nltk.FreqDist(tag for word, tag in tags)\n",
        "    return {\n",
        "        'nouns': pos_conts['NN'] + pos_conts['NNS'],\n",
        "       'verbs': pos_conts ['VB'] + pos_conts ['VBD'] + pos_conts ['VBG'] + pos_conts ['VBN'] + pos_conts ['VBP'] + pos_conts ['VBZ'],\n",
        "        'adjectvs': pos_conts ['JJ'] + pos_conts ['JJR'] + pos_conts ['JJS'],\n",
        "        'adverbs': pos_conts ['RB'] + pos_conts ['RBR'] + pos_conts ['RBS']\n",
        "    }\n",
        "feat_lis = X_train.apply(pos_feat)\n",
        "pos_featdata = pd.DataFrame(feat_lis.tolist())\n",
        "print(\"POS Tag Features:\\n\", pos_featdata)\n",
        "#polarity scores for review\n",
        "for review in reviews['text']:\n",
        "    blob = TextBlob(review)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    rating = \"positive\" if polarity > 0 else \"negative\"\n",
        "    print(f\"Review: '{review}' \\n Polarity Score: {polarity} rating: {rating}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOlQtdob0K4w",
        "outputId": "4d4960c0-25f0-40c4-8344-e699613f4061"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words [[1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0]\n",
            " [0 0 1 1 1 1 1 1 0 1 1 2 1 1 0 0 1 0 0 1 1 0 0 1 3 1 1 0 1 1 1 1 1 0 1 0\n",
            "  1 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            "  0 0 0 1 0 0]\n",
            " [0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 1]]\n",
            "TF-IDF Feature Matrix:\n",
            " [[0.5        0.         0.         0.         0.         0.\n",
            "  0.         0.         0.5        0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5\n",
            "  0.         0.         0.         0.5        0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.16303521 0.16303521 0.12853883 0.16303521\n",
            "  0.16303521 0.16303521 0.         0.16303521 0.16303521 0.32607042\n",
            "  0.16303521 0.16303521 0.         0.         0.16303521 0.\n",
            "  0.         0.16303521 0.16303521 0.         0.         0.16303521\n",
            "  0.48910563 0.16303521 0.16303521 0.         0.16303521 0.16303521\n",
            "  0.16303521 0.16303521 0.16303521 0.         0.16303521 0.\n",
            "  0.16303521 0.16303521 0.16303521 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.37796447 0.37796447 0.         0.\n",
            "  0.         0.         0.         0.         0.37796447 0.\n",
            "  0.         0.         0.         0.37796447 0.         0.\n",
            "  0.         0.         0.         0.37796447 0.         0.37796447\n",
            "  0.         0.         0.         0.37796447 0.         0.        ]\n",
            " [0.         0.46516193 0.         0.         0.36673901 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.46516193 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46516193 0.46516193]]\n",
            "N-grams Feature Matrix:\n",
            " [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0]\n",
            " [0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0\n",
            "  1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            "  1 1 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0\n",
            "  0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 1 1]]\n",
            "POS Tag Features:\n",
            "    nouns  verbs  adjectvs  adverbs\n",
            "0      1      1         1        1\n",
            "1     14     11        11        8\n",
            "2      3      3         0        2\n",
            "3      2      2         1        2\n",
            "Review: 'I highly recommend the VANMASS phone holder, and once you try one you will NEVER switch.' \n",
            " Polarity Score: 0.16 rating: positive\n",
            "Review: 'Very disappointed, not what I expected at all.' \n",
            " Polarity Score: -0.5375000000000001 rating: negative\n",
            "Review: 'Product feels very high quality. The suction to mount was very sticky and don't feel it will come undone. While driving a semi truck, it doesn't shake as much as other products. Very easy to use. Just have to get used to the release button on the back side. Good price for good product. Overall very happy with the product.' \n",
            " Polarity Score: 0.3607037037037037 rating: positive\n",
            "Review: 'Waste of money, doesn't work as advertised.' \n",
            " Polarity Score: -0.2 rating: negative\n",
            "Review: 'Fantastic performance, I absolutely love it!' \n",
            " Polarity Score: 0.5125 rating: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ],
      "metadata": {
        "id": "0XgkFOIE_KKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "df = pd.DataFrame(reviews)\n",
        "y_train = df['label']\n",
        "y_train_num = y_train.apply(lambda x: 1 if x == 'positive' else 0)\n",
        "t_vector = TfidfVectorizer()\n",
        "tf_feat = t_vector.fit_transform(df['text'])\n",
        "#using Chi-Square we can select the features\n",
        "k = 5  # Select the top 5 features\n",
        "chi2_selector = SelectKBest(chi2, k=k)\n",
        "chi2_feat = chi2_selector.fit_transform(tf_feat, y_train_num)\n",
        "# Get feature names and their scores\n",
        "feature_names = t_vector.get_feature_names_out()\n",
        "chi2_scores = chi2_selector.scores_\n",
        "# Create a DataFrame\n",
        "feature_rank = pd.DataFrame({'Feature': feature_names, 'Score': chi2_scores})\n",
        "feature_rank = feature_rank.sort_values(by='Score', ascending=False)\n",
        "#top-ranked features with the descending order\n",
        "print(\"Top Ranked Features:\\n\", feature_rank.head(k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpFThacZHY4w",
        "outputId": "ecb8ad5a-3d7e-4855-c8bf-296a19cd4d76"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Ranked Features:\n",
            "    Feature     Score\n",
            "18      of  0.581635\n",
            "15   money  0.581635\n",
            "2      all  0.581635\n",
            "34    work  0.581635\n",
            "32    what  0.581635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ],
      "metadata": {
        "id": "NTQuDwSQ_UsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "tokeniz = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "def get_embed(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embed = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "    return embed\n",
        "#this querry model try to look the querry features in the dataset\n",
        "query = \"High quality product with great performance.\"\n",
        "query_embed = get_embed(query, tokeniz, model)\n",
        "text_embed = [get_embed(text, tokeniz, model) for text in df['text']]\n",
        "similarr = [cosine_similarity(query_embed.reshape(1, -1), text_embed.reshape(1, -1))[0][0] for text_embed in text_embed]\n",
        "ranked_rev = sorted(zip(df['text'], similarr), key=lambda x: x[1], reverse=True)\n",
        "ranked_rev\n"
      ],
      "metadata": {
        "id": "udnlidxK_Vvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988fa5b5-0b15-44b6-97ff-854c3ef0e579"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Waste of money, doesn't work as advertised.\", 0.7220519),\n",
              " ('Very satisfied, works perfectly as described.', 0.7068167),\n",
              " ('Fantastic performance, I absolutely love it!', 0.58308506),\n",
              " ('I highly recommend the VANMASS phone holder, and once you try one you will NEVER switch.',\n",
              "  0.5686836),\n",
              " ('Very disappointed, not what I expected at all.', 0.5470326)]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "0XT272fr_aFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "the exercise was challenging,i have zero knowledge about the concept, this exercise made me understand about the concept and faced alot in understanding the concept the extraction features used related to nlp'''"
      ],
      "metadata": {
        "id": "swAKlSDD_bTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}